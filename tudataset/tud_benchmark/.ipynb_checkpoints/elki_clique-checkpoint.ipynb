{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "ed5ef45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from subprocess import Popen, PIPE\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "DATA_FILE_NAME = \"data.tsv\"\n",
    "# Install Java and download the elki bundle https://elki-project.github.io/releases/release0.7.5/elki-bundle-0.7.5.jar\n",
    "ELKI_JAR = \"elki-bundle-0.7.5.jar\"\n",
    "\n",
    "\n",
    "def elki_clique(X, tau= 0.2, xsi = 10, prune = \"true\"):\n",
    "    \"\"\"Perform COPAC clustering implemented by ELKI package.\n",
    "       The function calls jar package, which must be accessible through the\n",
    "       path stated in ELKI_JAR constant.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape (n_samples, n_features)\n",
    "            A feature array.\n",
    "        tau : float, optional, default=0.2\n",
    "            Clustering threshold for determining dense units. A unit in the CLIQUE grid is considered dense if it contains more than this fraction of the total points. \n",
    "        xsi : int, optional, default=10\n",
    "            Number of subdivision in the CLIQUE grid cluster. Each dimension is split into xsi number of equal intervals.\n",
    "        prune: string, optional, default=true\n",
    "            Use pruning during the implementation of the algorithm to speed up computation. Use \"false\" to return all clusters.\n",
    "       \n",
    "        Returns\n",
    "        -------\n",
    "        clusters : dict \n",
    "            Dictionary of clusters with their dimensions, extent(dense unit intervals) and point labels.\n",
    "        predictions : dict\n",
    "            Dictionary of prediction with dimensions as key and Y predictions as value. Outliers are labelled with -1.\n",
    "    \"\"\"\n",
    "    # write data into tsv file\n",
    "    np.savetxt(DATA_FILE_NAME, X, delimiter=\",\", fmt=\"%.6f\")\n",
    "    print(\"Run elki\")\n",
    "    # run elki with java\n",
    "    # You can find the read of the names of the parameters for the COPAC algorithm from the elki GUI\n",
    "    process = Popen([\"java\", \"-cp\", ELKI_JAR, \"de.lmu.ifi.dbs.elki.application.KDDCLIApplication\",\n",
    "                     \"-algorithm\", \"clustering.subspace.CLIQUE\",\n",
    "                     \"-dbc.in\", \"data.tsv\",\n",
    "                     \"-parser.colsep\", \",\",\n",
    "                     \"-clique.xsi\", str(xsi),\n",
    "                     \"-clique.tau\", str(tau),\n",
    "                     \"-clique.prune\", str(prune)],\n",
    "                    stdout=PIPE)\n",
    "    (output, err) = process.communicate()\n",
    "    exit_code = process.wait()\n",
    "    if exit_code != 0:\n",
    "        raise IOError(\"Elki implementation failed to execute: \\n {}\".format(output.decode(\"utf-8\")))\n",
    "\n",
    "    # remove data file\n",
    "    os.remove(DATA_FILE_NAME)\n",
    "\n",
    "    # parse output\n",
    "    elki_output = output.decode(\"utf-8\")\n",
    "    #print(elki_output)\n",
    "    # initialize array of ids and labels\n",
    "    # for each cluster, split by regex from output\n",
    "    clusters = []\n",
    "    \n",
    "    for i, cluster in enumerate(elki_output.split(\"Cluster: Cluster\")[1:]):\n",
    "        cluster_info = {}\n",
    "        cluster_info[\"id\"] = i\n",
    "        # find point coordinates in output\n",
    "        IDs_list = re.findall(r\"ID=(\\d+)\", cluster)\n",
    "        extent = re.findall(r\"d(\\d):\\[(\\-*\\d+\\.\\d+)\\;\\s(\\-*\\d+\\.\\d+)\\)\",cluster)\n",
    "        \n",
    "        dimensions = re.findall(r\"Dimensions: \\[(.*)\\]\",cluster)\n",
    "        #print(dimensions)\n",
    "        split_string = r\",\\s\"\n",
    "        dimensions = tuple(re.split(split_string,dimensions[0]))\n",
    "        #print(dimensions)\n",
    "        \n",
    "        cluster_info[\"dimension\"] = dimensions\n",
    "        cluster_info[\"extent\"] = extent\n",
    "        #print(\"cluster: {}\".format(i))\n",
    "        # create a numpy array\n",
    "        IDs = np.array(IDs_list, dtype=\"i\").reshape(-1, 1)\n",
    "        # append label\n",
    "        IDs_and_labels = np.hstack((IDs, np.repeat(i, len(IDs_list)).reshape(-1, 1)))\n",
    "        # append to matrix\n",
    "        cluster_info[\"id_and_label\"] = IDs_and_labels\n",
    "        clusters.append(cluster_info)\n",
    "        #print(IDs_and_labels)\n",
    "        #Y_pred = np.array([]).reshape(0, 2)\n",
    "        #Y_pred = np.vstack((Y_pred, IDs_and_labels))\n",
    "        #print(Y_pred)\n",
    "    prediction = {}\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        #print(\"Dimension:\")\n",
    "        #print(cluster[\"dimension\"])\n",
    "        #print(\"Id and label size:\")\n",
    "        #print(cluster[\"id_and_label\"].size)\n",
    "        #print(cluster[\"id_and_label\"])\n",
    "        dimension = cluster[\"dimension\"]\n",
    "        if dimension not in prediction.keys():\n",
    "            prediction[dimension] = cluster[\"id_and_label\"]\n",
    "        else:\n",
    "            prediction[dimension] = np.vstack((prediction[dimension],cluster[\"id_and_label\"]))\n",
    "    \n",
    "    point_counts = {}\n",
    "    for dimension,labels in prediction.items():\n",
    "        print(labels[:,0].size)\n",
    "        if dimension not in point_counts.keys():\n",
    "            point_counts[dimension] = labels[:,0].size\n",
    "        else:\n",
    "            point_counts[dimension] += labels[:,0].size\n",
    "        Y_preds = np.full((x.shape[0],1),-1)\n",
    "        #Y_preds[:,0] = np.arange(x.shape[0])\n",
    "        Y_preds[labels[:,0]-1,0] = labels[:,1]\n",
    "        prediction[dimension] = Y_preds\n",
    "\n",
    "    print(\"Found {} clusters in {} subspaces\".format(len(clusters),len(prediction)))\n",
    "    print(point_counts)\n",
    "    return clusters, prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "af5b114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run elki\n",
      "500\n",
      "500\n",
      "500\n",
      "448\n",
      "500\n",
      "498\n",
      "448\n",
      "402\n",
      "400\n",
      "302\n",
      "Found 30 clusters in 10 subspaces\n",
      "{('1',): 500, ('3',): 500, ('10',): 500, ('6',): 448, ('1', '3'): 500, ('1', '10'): 498, ('3', '6'): 448, ('3', '10'): 402, ('1', '3', '10'): 400, ('3', '6', '10'): 302}\n"
     ]
    }
   ],
   "source": [
    "x, y = make_blobs(n_samples=500,\n",
    "                  n_features=10,\n",
    "                  centers=5,\n",
    "                  cluster_std=0.1,\n",
    "                  )\n",
    "[clusters, prediction] = elki_clique(x, tau = 0.2, xsi = 5, prune = \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "f2c874e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: ('1',)\n",
      "Found 2 clusters with labels: [0 1]\n",
      "NMI: 0.5897 \n",
      "\n",
      "Dimensions: ('3',)\n",
      "Found 2 clusters with labels: [2 3]\n",
      "NMI: 0.5897 \n",
      "\n",
      "Dimensions: ('10',)\n",
      "Found 2 clusters with labels: [4 5]\n",
      "NMI: 0.5897 \n",
      "\n",
      "Dimensions: ('6',)\n",
      "Found 3 clusters with labels: [-1  6  7]\n",
      "NMI: 0.5658 \n",
      "\n",
      "Dimensions: ('1', '3')\n",
      "Found 5 clusters with labels: [ 8  9 10 11 12]\n",
      "NMI: 1.0000 \n",
      "\n",
      "Dimensions: ('1', '10')\n",
      "Found 5 clusters with labels: [-1 13 14 15 16]\n",
      "NMI: 0.9008 \n",
      "\n",
      "Dimensions: ('3', '6')\n",
      "Found 3 clusters with labels: [-1 17 18]\n",
      "NMI: 0.5658 \n",
      "\n",
      "Dimensions: ('3', '10')\n",
      "Found 5 clusters with labels: [-1 19 20 21 22]\n",
      "NMI: 0.9878 \n",
      "\n",
      "Dimensions: ('1', '3', '10')\n",
      "Found 5 clusters with labels: [-1 23 24 25 26]\n",
      "NMI: 1.0000 \n",
      "\n",
      "Dimensions: ('3', '6', '10')\n",
      "Found 4 clusters with labels: [-1 27 28 29]\n",
      "NMI: 0.8934 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dim,y_pred in prediction.items():\n",
    "    print(\"Dimensions: {}\".format(dim))\n",
    "    unique_labels = np.unique(y_pred)\n",
    "    print(\"Found {} clusters with labels: {}\".format(len(unique_labels),unique_labels))\n",
    "    nmi = normalized_mutual_info_score(y,y_pred.flatten())\n",
    "    print(f\"NMI: {nmi:.4f} \\n\")\n",
    "    \n",
    "#pred_and_y = np.concatenate([pred[:,None],y[:,None]],axis=1)\n",
    "#np.savetxt(\"results.csv\", pred_and_y, delimiter=\",\", fmt=\"%.1f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda3d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
